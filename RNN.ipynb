{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEE_hw3_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeXHWdIPA/67td+msIxGVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnisafazla/Neural_Networks_Implementations/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qpwy2bvCYM1"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import linspace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDZU_OBaEV0T",
        "outputId": "6c86aa41-3411-40b2-bf65-ba0b61d230f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = h5py.File(\"/content/drive/MyDrive/EEE443/hw3/assign3_data3.h5\", \"r\")\n",
        "X_train = np.array(file['trX'])\n",
        "X_test = np.array(file['tstX'])\n",
        "Y_train = np.array(file['trY'])\n",
        "Y_test = np.array(file['tstY'])\n",
        "\n",
        "index = [*range(X_train.shape[0])]\n",
        "random.shuffle(index)\n",
        "X_train = np.array([X_train[i] for i in index])\n",
        "Y_train = np.array([Y_train[i] for i in index])\n",
        "\n",
        "index = [*range(X_test.shape[0])]\n",
        "random.shuffle(index)\n",
        "X_test = np.array([X_test[i] for i in index])\n",
        "Y_test = np.array([Y_test[i] for i in index])\n",
        "\n",
        "n_val_x = int(X_train.shape[0] * 0.1)\n",
        "X_val = X_train[0:n_val_x]\n",
        "X_train = X_train[n_val_x:]\n",
        "Y_val = Y_train[0:n_val_x]\n",
        "Y_train = Y_train[n_val_x:]\n",
        "\n",
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape, X_val.shape, Y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajCALh29EbML",
        "outputId": "b35bcf2d-008a-4c4f-a959-0f8f8490d505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2700, 150, 3) (600, 150, 3) (2700, 6) (600, 6) (300, 150, 3) (300, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "momentum = 0.85\n",
        "batch_size = 32                  \n",
        "in_shape = (150,3)\n",
        "n_classes = 6\n",
        "n_units = 128\n",
        "n_hidden1 = 100\n",
        "n_hidden2 = 50\n",
        "# other hidden layers here\n",
        "max_epochs = 20\n",
        "# extra hyperparameters\n",
        "min_clip_value = -10\n",
        "max_clip_value = 10\n",
        "bptt_truncate = 5"
      ],
      "metadata": {
        "id": "Jo5Lk9eBFZup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_entropy(W_in, W_rec, W_hid1, W_hid2, W_out, b_in, b_rec, b_hid1, b_hid2, b_out, X, Y):\n",
        "  c_matrix = np.zeros((n_classes, n_classes))\n",
        "  N = Y.shape[0]\n",
        "  prev_v = np.zeros((N, n_units))\n",
        "  for t in range(in_shape[0]):\n",
        "    new_input = X[:,t,:]  # N,3\n",
        "    # print(new_input.shape)\n",
        "    mul_in = np.dot(new_input, W_in.T) + b_in\n",
        "    # print(mul_in.shape)\n",
        "    mul_rec = np.dot(prev_v, W_rec.T) + b_rec\n",
        "    add = mul_rec + mul_in\n",
        "    v = tanh(add)\n",
        "    mul_hid1 = np.dot(v, W_hid1.T) + b_hid1\n",
        "    out_hid1 = tanh(mul_hid1)\n",
        "    mul_hid2 = np.dot(mul_hid1, W_hid2.T) + b_hid2\n",
        "    out_hid2 = tanh(mul_hid2)\n",
        "    mul_out = np.dot(out_hid2, W_out.T) + b_out\n",
        "    pred = softmax(mul_out)\n",
        "    prev_v = v       \n",
        "\n",
        "  loss = - np.sum(Y * np.log2(pred)) / N\n",
        "  for sample in range(pred.shape[0]):\n",
        "    c_matrix[np.argmax(pred[sample]), np.argmax(Y[sample])] += 1\n",
        "  return loss, c_matrix\n",
        "# SOFTMAX ACTIVATION FUNCTION\n",
        "def softmax(X):\n",
        "  x_max = np.array([np.max(row) for row in X])\n",
        "  sum = np.array([np.sum(np.exp(row - np.max(row))) for row in X])\n",
        "  prob = np.array([np.exp(X[i] - np.max(X[i])) / sum[i] for i in range(X.shape[0])])\n",
        "  return prob\n"
      ],
      "metadata": {
        "id": "lqvk7g-MAhoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cross_entropy(model.W_in, model.W_rec, model.W_hid1, model.W_out, model.b_in, model.b_rec, model.b_hid1, model.b_out, X_val, Y_val)"
      ],
      "metadata": {
        "id": "WcXv-wx2FUM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return 1.0 - np.tanh(x)**2\n",
        "\n",
        "def xavier(n_samples):\n",
        "  lower, upper = -(1.0 / np.sqrt(n_samples)), (1.0 / np.sqrt(n_samples))\n",
        "  numbers = np.random.rand(n_samples)\n",
        "  return lower + numbers * (upper - lower)\n",
        "\n",
        "def plot_history(train, val):\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(train, label='train_loss')\n",
        "  plt.plot(val, label='val_loss')\n",
        "  plt.legend()\n",
        "  return plt.figure"
      ],
      "metadata": {
        "id": "FLAavZj2FkOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip(dW_in, dW_rec, dW_hid1, dW_hid2, dW_out, db_in, db_rec, db_hid1, db_hid2, db_out):\n",
        "  # clip the values\n",
        "  if dW_in.max() > max_clip_value:\n",
        "    dW_in[dW_in > max_clip_value] = max_clip_value\n",
        "  if dW_out.max() > max_clip_value:\n",
        "    dW_out[dW_out > max_clip_value] = max_clip_value\n",
        "  if dW_rec.max() > max_clip_value:\n",
        "    dW_rec[dW_rec > max_clip_value] = max_clip_value\n",
        "  if dW_hid1.max() > max_clip_value:\n",
        "    dW_hid1[dW_hid1 > max_clip_value] = max_clip_value\n",
        "  if dW_hid2.max() > max_clip_value:\n",
        "    dW_hid2[dW_hid2 > max_clip_value] = max_clip_value\n",
        "      \n",
        "  \n",
        "  if dW_in.min() < min_clip_value:\n",
        "    dW_in[dW_in < min_clip_value] = min_clip_value\n",
        "  if dW_out.min() < min_clip_value:\n",
        "    dW_out[dW_out < min_clip_value] = min_clip_value\n",
        "  if dW_rec.min() < min_clip_value:\n",
        "    dW_rec[dW_rec < min_clip_value] = min_clip_value\n",
        "  if dW_hid1.min() < min_clip_value:\n",
        "    dW_hid1[dW_hid1 < min_clip_value] = min_clip_value\n",
        "  if dW_hid2.min() < min_clip_value:\n",
        "    dW_hid2[dW_hid2 < min_clip_value] = min_clip_value\n",
        "\n",
        "  if db_in.max() > max_clip_value:\n",
        "    db_in[db_in > max_clip_value] = max_clip_value\n",
        "  if db_out.max() > max_clip_value:\n",
        "    db_out[db_out > max_clip_value] = max_clip_value\n",
        "  if db_rec.max() > max_clip_value:\n",
        "    db_rec[db_rec > max_clip_value] = max_clip_value\n",
        "  if db_hid1.max() > max_clip_value:\n",
        "    db_hid1[db_hid1 > max_clip_value] = max_clip_value\n",
        "  if db_hid2.max() > max_clip_value:\n",
        "    db_hid2[db_hid2 > max_clip_value] = max_clip_value\n",
        "  \n",
        "  if db_in.min() < min_clip_value:\n",
        "    db_in[db_in < min_clip_value] = min_clip_value\n",
        "  if db_out.min() < min_clip_value:\n",
        "    db_out[db_out < min_clip_value] = min_clip_value\n",
        "  if db_rec.min() < min_clip_value:\n",
        "    db_rec[db_rec < min_clip_value] = min_clip_value\n",
        "  if db_hid1.min() < min_clip_value:\n",
        "    db_hid1[db_hid1 < min_clip_value] = min_clip_value\n",
        "  if db_hid2.min() < min_clip_value:\n",
        "    db_hid2[db_hid2 < min_clip_value] = min_clip_value\n",
        "\n",
        "  return dW_in, dW_rec, dW_hid1, dW_hid2, dW_out, db_in, db_rec, db_hid1, db_hid2, db_out"
      ],
      "metadata": {
        "id": "Nf3rbClmImLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(object):\n",
        "  def __init__(self):\n",
        "    self.W_in = xavier(n_units * in_shape[1]).reshape(n_units, in_shape[1])   # input to the layer\n",
        "    self.b_in = xavier(n_units)\n",
        "    self.rW_in = np.zeros(self.W_in.shape)\n",
        "    self.rb_in = np.zeros(self.b_in.shape)\n",
        "    self.W_rec = xavier(n_units * n_units).reshape(n_units, n_units)  # input from the past\n",
        "    self.b_rec = xavier(n_units)\n",
        "    self.rW_rec = np.zeros(self.W_rec.shape)\n",
        "    self.rb_rec = np.zeros(self.b_rec.shape)\n",
        "    # probably a hidden dense layer here\n",
        "    self.W_hid1 = xavier(n_hidden1 * n_units).reshape(n_hidden1, n_units) # first hidden layer of MLP\n",
        "    self.b_hid1 = xavier(n_hidden1)\n",
        "    self.rW_hid1 = np.zeros(self.W_hid1.shape)\n",
        "    self.rb_hid1 = np.zeros(self.b_hid1.shape)\n",
        "    # other hidden layers here\n",
        "    self.W_hid2 = xavier(n_hidden2 * n_hidden1).reshape(n_hidden2, n_hidden1) # first hidden layer of MLP\n",
        "    self.b_hid2 = xavier(n_hidden2)\n",
        "    self.rW_hid2 = np.zeros(self.W_hid2.shape)\n",
        "    self.rb_hid2 = np.zeros(self.b_hid2.shape)\n",
        "    self.W_out = xavier(n_classes * n_hidden2).reshape(n_classes, n_hidden2) # output layer\n",
        "    self.b_out = xavier(n_classes)\n",
        "    self.rW_out = np.zeros(self.W_out.shape)\n",
        "    self.rb_out = np.zeros(self.b_out.shape)\n",
        "    self.val_hist = []\n",
        "    self.train_hist = []\n",
        "  def train(self, X_train, Y_train, X_val, Y_val, s_epoch=0, s_batch=0):\n",
        "    # train and val data are already shuffled.\n",
        "    N = Y_train.shape[0]\n",
        "    n_batches = N // batch_size\n",
        "    extra = N % batch_size\n",
        "\n",
        "    for self.epoch in range(s_epoch,max_epochs):\n",
        "      self.c_matrix = np.zeros((n_classes, n_classes))  # confusion matrix for training\n",
        "      train_loss = 0\n",
        "      for self.batch in range(s_batch,n_batches):\n",
        "        train_loss_batch = 0\n",
        "        if self.batch < n_batches - 1:\n",
        "          net_batch_size = batch_size\n",
        "          batch_X = X_train[self.batch * batch_size:(self.batch+1) * batch_size]\n",
        "          batch_Y = Y_train[self.batch * batch_size:(self.batch+1) * batch_size]\n",
        "        else:\n",
        "          net_batch_size = batch_size + extra\n",
        "          batch_X = X_train[self.batch * batch_size:(self.batch+1) * batch_size + extra]\n",
        "          batch_Y = Y_train[self.batch * batch_size:(self.batch+1) * batch_size + extra]\n",
        "        # train model\n",
        "        dW_in = np.zeros(self.W_in.shape)\n",
        "        dW_out = np.zeros(self.W_out.shape)\n",
        "        dW_rec = np.zeros(self.W_rec.shape)\n",
        "        dW_hid1 = np.zeros(self.W_hid1.shape)\n",
        "        dW_hid2 = np.zeros(self.W_hid2.shape)\n",
        "        db_in = np.zeros(self.b_in.shape)\n",
        "        db_out = np.zeros(self.b_out.shape)\n",
        "        db_rec = np.zeros(self.b_rec.shape) \n",
        "        db_hid1 = np.zeros(self.b_hid1.shape) \n",
        "        db_hid2 = np.zeros(self.b_hid2.shape) \n",
        "        \n",
        "        dW_in_tmp = np.zeros(self.W_in.shape)\n",
        "        dW_out_tmp = np.zeros(self.W_out.shape)\n",
        "        dW_rec_tmp = np.zeros(self.W_rec.shape)\n",
        "        dW_hid1_tmp = np.zeros(self.W_hid1.shape)\n",
        "        dW_hid2_tmp = np.zeros(self.W_hid2.shape)\n",
        "        db_in_tmp = np.zeros(self.b_in.shape)\n",
        "        db_out_tmp = np.zeros(self.b_out.shape)\n",
        "        db_rec_tmp = np.zeros(self.b_rec.shape)\n",
        "        db_hid1_tmp = np.zeros(self.b_hid1.shape)\n",
        "        db_hid2_tmp = np.zeros(self.b_hid2.shape)\n",
        "\n",
        "        dW_in_t = np.zeros(self.W_in.shape)\n",
        "        dW_rec_t = np.zeros(self.W_rec.shape)\n",
        "        db_in_t = np.zeros(self.b_in.shape)\n",
        "        db_rec_t = np.zeros(self.b_rec.shape)\n",
        "        N = batch_X.shape[0]\n",
        "        # forward pass\n",
        "        layers = []\n",
        "        prev_v = np.zeros((N, n_units))\n",
        "        for t in range(in_shape[0]):\n",
        "          new_input = batch_X[:,t,:]  # N,3\n",
        "          # print(new_input.shape)\n",
        "          mul_in = np.dot(new_input, self.W_in.T) + self.b_in\n",
        "          # print(mul_in.shape)\n",
        "          mul_rec = np.dot(prev_v, self.W_rec.T) + self.b_rec\n",
        "          add = mul_rec + mul_in\n",
        "          v = tanh(add)\n",
        "          mul_hid1 = np.dot(v, self.W_hid1.T) + self.b_hid1\n",
        "          out_hid1 = tanh(mul_hid1)\n",
        "          mul_hid2 = np.dot(mul_hid1, self.W_hid2.T) + self.b_hid2\n",
        "          out_hid2 = tanh(mul_hid2)\n",
        "          mul_out = np.dot(out_hid2, self.W_out.T) + self.b_out\n",
        "          pred = softmax(mul_out)\n",
        "          layers.append({'v':v, 'prev_v':prev_v, 'out_hid1':out_hid1, 'out_hid2':out_hid2, 'add':add})\n",
        "          prev_v = v       \n",
        "\n",
        "        train_loss_batch = - np.sum(batch_Y * np.log2(pred)) / net_batch_size\n",
        "        train_loss += train_loss_batch\n",
        "        for sample in range(pred.shape[0]):\n",
        "          self.c_matrix[np.argmax(pred[sample]), np.argmax(batch_Y[sample])] += 1\n",
        "\n",
        "        # minimize categorical cross entropy\n",
        "        # derivative of softmax\n",
        "        dpred = (pred - batch_Y)\n",
        "        \n",
        "        # backward pass\n",
        "        for t in range(in_shape[0]):\n",
        "          dW_out_tmp = np.dot(dpred.T, layers[t]['out_hid2'])\n",
        "          db_out_tmp = np.dot(dpred.T, np.ones((N)))\n",
        "          err_out = np.dot(dpred, self.W_out)\n",
        "          delta_out = tanh_derivative(mul_hid2) * err_out\n",
        "\n",
        "          dW_hid2_tmp = np.dot(delta_out.T, layers[t]['out_hid1'])\n",
        "          db_hid2_tmp = np.dot(delta_out.T, np.ones((N)))\n",
        "          err_hid2 = np.dot(delta_out, self.W_hid2)\n",
        "          delta_hid2 = tanh_derivative(mul_hid1) * err_hid2\n",
        "\n",
        "          dW_hid1_tmp = np.dot(delta_hid2.T, layers[t]['v'])\n",
        "          db_hid1_tmp = np.dot(delta_hid2.T, np.ones((N)))\n",
        "          err_hid1 = np.dot(delta_hid2, self.W_hid1)\n",
        "          delta_hid1 = tanh_derivative(add) * err_hid1\n",
        "\n",
        "          # dmul_rec = delta_hid * np.ones_like(mul_rec)\n",
        "          # dprev_v = np.dot(np.transpose(self.W_rec), dmul_rec)\n",
        "          delta = delta_hid1\n",
        "          for i in range(t-1, max(-1, t-bptt_truncate-1), -1):\n",
        "            # error = err_hid1 + dprev_v\n",
        "            # delta = tanh_derivative(add) * error\n",
        "            new_input = batch_X[:,t,:]\n",
        "            dW_in_t = np.dot(delta.T, new_input)\n",
        "            db_in_t = np.dot(delta.T, np.ones((N)))\n",
        "\n",
        "            dW_rec_t = np.dot(delta.T, layers[t]['prev_v'])\n",
        "            dB_rec_t = np.dot(delta.T, np.ones((N)))\n",
        "\n",
        "            err_rec = np.dot(delta, self.W_rec)\n",
        "            delta = tanh_derivative(layers[t]['add']) * err_rec\n",
        "\n",
        "            dW_in_tmp += dW_in_t\n",
        "            dW_rec_tmp += dW_rec_t\n",
        "            db_in_tmp += db_in_t\n",
        "            db_rec_tmp += db_rec_t\n",
        "          dW_out += dW_out_tmp\n",
        "          dW_hid2 += dW_hid2_tmp\n",
        "          dW_hid1 += dW_hid1_tmp\n",
        "          dW_in += dW_in_tmp\n",
        "          dW_rec += dW_rec_tmp\n",
        "          db_out += db_out_tmp\n",
        "          db_hid2 += db_hid2_tmp\n",
        "          db_hid1 += db_hid1_tmp\n",
        "          db_in += db_in_tmp\n",
        "          db_rec += db_in_tmp\n",
        "          \n",
        "        # update momentum terms\n",
        "        dW_in, dW_rec, dW_hid1, dW_hid2, dW_out, db_in, db_rec, db_hid1, db_hid2, db_out = clip(dW_in, dW_rec, dW_hid1, dW_hid2, dW_out, db_in, db_rec, db_hid1, db_hid2, db_out)\n",
        "\n",
        "        self.rW_in = momentum * self.rW_in + (1 - momentum) * (dW_in / net_batch_size) ** 2\n",
        "        self.rW_out = momentum * self.rW_out + (1 - momentum) * (dW_out / net_batch_size) ** 2\n",
        "        self.rW_rec = momentum * self.rW_rec + (1 - momentum) * (dW_rec / net_batch_size) ** 2\n",
        "        self.rW_hid1 = momentum * self.rW_hid1 + (1 - momentum) * (dW_hid1 / net_batch_size) ** 2\n",
        "        self.rW_hid2 = momentum * self.rW_hid2 + (1 - momentum) * (dW_hid2 / net_batch_size) ** 2\n",
        "        self.rb_in = momentum * self.rb_in + (1 - momentum) * (db_in / net_batch_size) ** 2\n",
        "        self.rb_out = momentum * self.rb_out + (1 - momentum) * (db_out / net_batch_size) ** 2\n",
        "        self.rb_rec = momentum * self.rb_rec + (1 - momentum) * (db_rec / net_batch_size) ** 2\n",
        "        self.rb_hid1 = momentum * self.rb_hid1 + (1 - momentum) * (db_hid1 / net_batch_size) ** 2\n",
        "        self.rb_hid2 = momentum * self.rb_hid2 + (1 - momentum) * (db_hid2 / net_batch_size) ** 2\n",
        "\n",
        "        # update after each mini-batch\n",
        "        self.W_in -= learning_rate * dW_in / net_batch_size / np.sqrt(0.0000000000000000001 + self.rW_in)\n",
        "        self.W_out -= learning_rate * dW_out / net_batch_size / np.sqrt(0.0000000000000000001 + self.rW_out)\n",
        "        self.W_rec -= learning_rate * dW_rec / net_batch_size / np.sqrt(0.0000000000000000001 + self.rW_rec)\n",
        "        self.W_hid1 -= learning_rate * dW_hid1 / net_batch_size / np.sqrt(0.0000000000000000001 + self.rW_hid1)\n",
        "        self.W_hid2 -= learning_rate * dW_hid2 / net_batch_size / np.sqrt(0.0000000000000000001 + self.rW_hid2)\n",
        "        self.b_in -= learning_rate * db_in / net_batch_size / np.sqrt(0.0000000000000000001 + self.rb_in)\n",
        "        self.b_out -= learning_rate * db_out / net_batch_size / np.sqrt(0.0000000000000000001 + self.rb_out)\n",
        "        self.b_rec -= learning_rate * db_rec / net_batch_size / np.sqrt(0.0000000000000000001 + self.rb_rec)\n",
        "        self.b_hid1 -= learning_rate * db_hid1 / net_batch_size / np.sqrt(0.0000000000000000001 + self.rb_hid1)\n",
        "        self.b_hid2 -= learning_rate * db_hid2 / net_batch_size / np.sqrt(0.0000000000000000001 + self.rb_hid2)\n",
        "\n",
        "        # print('Epoch ', self.epoch+1, ', batch ', self.batch+1, ' / ', n_batches, ' => ', 'train_loss=', train_loss_batch)\n",
        "      train_loss = train_loss / (n_batches - s_batch)\n",
        "      val_loss, _ = categorical_cross_entropy(self.W_in, self.W_rec, self.W_hid1, self.W_hid2, self.W_out, self.b_in, self.b_rec, self.b_hid1, self.b_hid2, self.b_out, X_val, Y_val)\n",
        "      self.train_hist.append(train_loss)\n",
        "      self.val_hist.append(val_loss)\n",
        "      print('Epoch ', str(self.epoch+1), ' => train_loss=', train_loss, ', val_loss=', val_loss)\n",
        "      # if val_loss < val_loss_threshold:  # ???\n",
        "      #  return train_hist, val_hist, W_in, b_in, W_rec, b_rec, W_out, b_out\n",
        "      \n",
        "    return self.train_hist, self.val_hist"
      ],
      "metadata": {
        "id": "-nV7rvCNSoRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()"
      ],
      "metadata": {
        "id": "WotlwwVJtitN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.rW_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXHV3SLjVIqt",
        "outputId": "263c0d94-8842-4a86-8fc5-14930fd33d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mul = model.train(X_train, Y_train, X_val, Y_val, s_batch=0, s_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_d88l5itpSG",
        "outputId": "838fe1b4-34c9-4fd2-b691-4ca6f65207d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1  => train_loss= 7.153472167541775 , val_loss= 8.037330531658023\n",
            "Epoch  2  => train_loss= 9.77278636617864 , val_loss= 9.7389848808166\n",
            "Epoch  3  => train_loss= 11.27577290844025 , val_loss= 5.168801362059048\n",
            "Epoch  4  => train_loss= 6.77520658364905 , val_loss= 5.339282971683203\n",
            "Epoch  5  => train_loss= 6.044216402431624 , val_loss= 3.2143601225850262\n",
            "Epoch  6  => train_loss= 6.347235954446859 , val_loss= 5.150225938396729\n",
            "Epoch  7  => train_loss= 6.136491876757911 , val_loss= 4.279493828069977\n",
            "Epoch  8  => train_loss= 6.150002241181289 , val_loss= 6.4219614948854975\n",
            "Epoch  9  => train_loss= 6.057261230986258 , val_loss= 5.516124049178367\n",
            "Epoch  10  => train_loss= 6.056868084827833 , val_loss= 5.142353638339502\n"
          ]
        }
      ]
    }
  ]
}