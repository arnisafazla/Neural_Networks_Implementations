{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEEhw3_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdLpp3rU4VYfiAC1XhBFj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnisafazla/Neural_Networks_Implementations/blob/main/encoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aXPyoi-L4VK"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import linspace\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DoPRfrXMDaY",
        "outputId": "aee82866-cd55-4d34-b272-f30c3d9505d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = h5py.File(\"/content/drive/MyDrive/EEE443/hw3/assign3_data1.h5\", \"r\")\n",
        "data = file['data']\n",
        "data = np.array([np.stack((sample[0], sample[1], sample[2]), axis=2) for sample in data])"
      ],
      "metadata": {
        "id": "rfs4jz11MPXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "  def __init__(self, data, to_grayscale=True, normalize=True):\n",
        "    self.raw_data = np.array(data)\n",
        "    processed_data = data\n",
        "    if to_grayscale:\n",
        "      processed_data = self.to_grayscale(processed_data)\n",
        "    if normalize:\n",
        "      processed_data = self.normalize_data(processed_data)\n",
        "    self.data = np.array(processed_data)\n",
        "  def to_grayscale(self, data):\n",
        "    R = data.T[0].T\n",
        "    G = data.T[1].T\n",
        "    B = data.T[2].T\n",
        "    return R * 0.2126 + G * 0.7152 + B * 0.0722\n",
        "  def normalize_data(self, data):\n",
        "    data = data - np.mean(data)\n",
        "    data = np.clip(data, data.mean() - 3 * data.std(), data.mean() + 3 * data.std())\n",
        "    data = (data - data.min()) / (data.max() - data.min())\n",
        "    return data * (0.9 - 0.1) + 0.1\n",
        "  def visualize(self, indices):\n",
        "    # no_of_images should be 10 * k\n",
        "    img_ver_raw = []\n",
        "    img_hor_raw = []\n",
        "    img_ver = []\n",
        "    img_hor = []\n",
        "    for sample in range(indices.shape[0]):\n",
        "      normed = cv2.normalize(self.raw_data[indices[sample]], None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "      resized = cv2.resize(normed, (100, 100))\n",
        "      img_hor_raw.append(resized)\n",
        "\n",
        "      normed = cv2.normalize(self.data[indices[sample]], None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "      resized = cv2.resize(normed, (100, 100))\n",
        "      img_hor.append(resized)\n",
        "      if (sample + 1) % 10 == 0:\n",
        "        img_ver_raw.append(img_hor_raw)\n",
        "        img_hor_raw = []\n",
        "        img_ver.append(img_hor)\n",
        "        img_hor = []\n",
        "\n",
        "    cv2.imwrite('raw_images.png', cv2.vconcat([cv2.hconcat(row) for row in img_ver_raw]))\n",
        "    cv2.imwrite('processed_images.png', cv2.vconcat([cv2.hconcat(row) for row in img_ver]))"
      ],
      "metadata": {
        "id": "wlItOemxM_lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset(data)"
      ],
      "metadata": {
        "id": "_saKN9rMqlLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.random.randint(0,data.shape[0],200)\n",
        "dataset.visualize(indices)"
      ],
      "metadata": {
        "id": "L72VdBsfsELn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer (object):\n",
        "  def __init__(self, no_of_units, input_shape, init_type, init_std, init_range, learning_rate):\n",
        "    self.no_of_units = no_of_units\n",
        "    if init_type == 'Gaussian':\n",
        "      self.W = np.random.normal(0, init_std, no_of_units * (input_shape + 1)).reshape((no_of_units, input_shape + 1))\n",
        "    elif init_type == 'uniform':\n",
        "      self.W = np.random.uniform(low=init_range[0], high=init_range[1], size=no_of_units * input_shape).reshape((no_of_units, input_shape))\n",
        "      self.b = np.random.uniform(low=init_range[0], high=init_range[1], size=no_of_units)\n",
        "    # W has b as well\n",
        "    self.learning_rate = learning_rate\n",
        "    self.alpha = alpha\n",
        "    self.update = np.zeros(self.W.shape)\n",
        "    self.r = np.zeros(self.W.shape)\n",
        "    return\n",
        "    # alpha and r for RMSProp optimization.\n",
        "\n",
        "  def forward_pass(self, X):\n",
        "    # print('in forward pass')\n",
        "    # print(\"x: \", X.shape)\n",
        "    X_e = np.array([np.append(row, -1) for row in X])\n",
        "    # print(\"X_e, self.W.T: \")\n",
        "    # print(X_e.shape, self.W.T.shape)\n",
        "    v = np.matmul(X_e, self.W.T)\n",
        "    # save for backpropagation\n",
        "    self.X = X_e\n",
        "    self.v = v\n",
        "    # print(\"tanh(v): \")\n",
        "    # print(tanh(v).shape)\n",
        "    return tanh(v)\n",
        "  def back_propogate(self, error, batch_size):\n",
        "    # print(\"error: \", error)\n",
        "    delta = np.array([error[i] * tanh_derivative(self.v[i]) for i in range(self.v.shape[0])])\n",
        "    # print(\"delta: \", delta)\n",
        "    # print(\"X: \", self.X)\n",
        "    gradient = np.matmul(delta.T, self.X)\n",
        "    self.r = self.alpha * self.r + (1 - self.alpha) * gradient ** 2\n",
        "    gradient_sum = np.sum(np.abs(gradient))\n",
        "    # print(\"     W: \", self.W.shape)\n",
        "    # print(\"gradient: \", self.gradient)\n",
        "    self.W_update = self.learning_rate * gradient / (0.0000000000000001 + np.sqrt(self.r))\n",
        "    self.W_update = self.W_update / batch_size  # take average of the W updates\n",
        "    # print(\"W_update: \", self.W_update)\n",
        "    W_raw = self.W.T[0:self.W.shape[-1] - 1].T\n",
        "    self.W = self.W + self.W_update \n",
        "    # print(\"     W_raw: \", W_raw.shape)\n",
        "    # print(\"     delta: \", delta.shape)\n",
        "    next_error = np.matmul(delta, W_raw)\n",
        "    return next_error, gradient_sum"
      ],
      "metadata": {
        "id": "8t03eui7RSyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoderModel(object):\n",
        "  def __init__(self, layers, units, input_shape, init_type='Gaussian', init_std=0.1, init_range=(-0.1,0.1), learning_rate=0.1):\n",
        "    self.layers = []\n",
        "    self.layers.append(DenseLayer(units[0], input_shape=input_shape, init_type=init_type, init_std=init_std, init_range=init_range, learning_rate=learning_rate))\n",
        "    for i in range(layers-1):\n",
        "      self.layers.append(DenseLayer(units[i+1], input_shape=units[i], init_type=init_type, init_std=init_std, init_range=init_range, learning_rate=learning_rate))\n",
        "    self.no_of_layers = layers\n",
        "    self.input_shape = input_shape\n",
        "\n",
        "  def train(self, X_train, Y_train, X_test, Y_test, batch_size, max_epochs, verbose=0):\n",
        "    MSE_train_hist = []\n",
        "    MCE_train_hist = []\n",
        "    MSE_test_hist = []\n",
        "    MCE_test_hist = []\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    no_of_batches = n // batch_size\n",
        "    extra = n % batch_size\n",
        "\n",
        "    # shuffle X_test, Y_Test once\n",
        "    index = [*range(Y_test.shape[0])]\n",
        "    random.shuffle(index)\n",
        "    X_test_shuffled = np.array([X_test[i] for i in index])\n",
        "    Y_test_shuffled = np.array([Y_test[i] for i in index])\n",
        "\n",
        "    for epoch in range(max_epochs):   # ??\n",
        "      # for loop to see the smallest error\n",
        "      gradient = 0\n",
        "      MSE_train = 0\n",
        "      MCE_train = 0\n",
        "\n",
        "      # shuffle X_train, Y_train\n",
        "      index = [*range(n)]\n",
        "      random.shuffle(index)\n",
        "      X_shuffled = np.array([X_train[i] for i in index])\n",
        "      Y_shuffled = np.array([Y_train[i] for i in index])\n",
        "\n",
        "      for i in range(no_of_batches):\n",
        "        if i < no_of_batches - 1:\n",
        "          net_batch_size = batch_size\n",
        "          batch_X = X_shuffled[i * batch_size:(i+1) * batch_size]\n",
        "          batch_Y = Y_shuffled[i * batch_size:(i+1) * batch_size]\n",
        "          \n",
        "        else:\n",
        "          net_batch_size = batch_size + extra\n",
        "          batch_X = X_shuffled[i * batch_size:(i+1) * batch_size + extra]\n",
        "          batch_Y = Y_shuffled[i * batch_size:(i+1) * batch_size + extra]\n",
        "          \n",
        "        for layer in self.layers:\n",
        "          batch_X = layer.forward_pass(batch_X)\n",
        "        MSE_train = MSE_train + MSE(batch_X, batch_Y)\n",
        "        MCE_train = MCE_train + MCE(batch_X, batch_Y)\n",
        "        # return batch_X, batch_Y\n",
        "        layer_error = error(batch_X, batch_Y)\n",
        "        # print(\"output layer error: \", layer_error)\n",
        "        for i in range(self.no_of_layers-1, -1, -1):\n",
        "          # try:\n",
        "          layer_error, grad = self.layers[i].back_propogate(layer_error, net_batch_size)\n",
        "          gradient += grad\n",
        "          #except:\n",
        "          #  return \n",
        "          # print(\"hidden layer error: \", layer_error)\n",
        "          # in backpropogate update the Ws and return W.T * delta for the next layer\n",
        "      MSE_train_hist.append(MSE_train / no_of_batches)\n",
        "      MCE_train_hist.append(MCE_train / no_of_batches)\n",
        "      MSE_test, MCE_test = self.test(X_test_shuffled, Y_test_shuffled)\n",
        "      MSE_test_hist.append(MSE_test)\n",
        "      MCE_test_hist.append(MCE_test)\n",
        "      if verbose:\n",
        "        print(\"Epoch \", epoch, \" => MSE_train: \", MSE_train, \", MCE_train: \", MCE_train, \", MSE_test: \", MSE_test, \", MCE_test: \", MCE_test, \" gradient: \", gradient)\n",
        "    return MSE_train_hist, MCE_train_hist, MSE_test_hist, MCE_test_hist\n",
        "\n",
        "  def test(self, X, Y):\n",
        "    X_tmp = X\n",
        "    for layer in self.layers:\n",
        "      X_tmp = layer.forward_pass(X_tmp)\n",
        "      # X_tmp has the output now\n",
        "    return MSE(X_tmp, Y), MCE(X_tmp, Y)"
      ],
      "metadata": {
        "id": "7FG4WYht2FMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L_pre = L_hid = 64\n",
        "L_post = 50\n",
        "l = 5 * 10 ** (-4)\n",
        "beta = 0.1\n",
        "rho = 0.1\n",
        "L_in = input_shape = data.shape[1] * data.shape[2]\n",
        "w0 = np.sqrt(6 / (L_pre + L_post))\n",
        "learning_rate = 0.1\n",
        "autoencoder = Model(layers=2, units=[L_pre, L_post], input_shape=input_shape, init_type='uniform', init_range=(-w0,w0), learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "pXI285j9nO4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = np.random.uniform(low=-w0, high=w0, size=L_pre * Lin).reshape((L_pre, Lin))\n",
        "b1 = np.random.uniform(low=-w0, high=w0, size=L_pre)\n",
        "W2 = np.random.uniform(low=-w0, high=w0, size=L_post * Lhid).reshape((L_pre, Lin))\n",
        "b2 = np.random.uniform(low=-w0, high=w0, size=L_post)"
      ],
      "metadata": {
        "id": "Ad8RqeFXY3od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aeCost(We, data, params):\n",
        "  # We = [W1 W2 b1 b2]\n",
        "  # data: (input_shape, N) : literally data\n",
        "  # params: Lin, Lhid, l (lambda), beta, rho\n",
        "  # Lin : input_shape, Lhid: L_pre, Lout: L_post\n",
        "  # assume linear activation function for now\n",
        "  Lin, Lhid, l, beta, rho = params\n",
        "  W1, W2, b1, b2 = We\n",
        "  N = data.shape[0]\n",
        "  X = data.reshape((N, data[1] * data[2]))\n",
        "  hidden_signal = np.matmul(X, W1.T) - b1\n",
        "  # hidden_signal[hidden_signal < 0] = 0        # relu activation\n",
        "  output = np.matmul(hidden_signal, W2.T) - b2\n",
        "  first_term = np.sum((output - X) ** 2) / (2 * N)\n",
        "  second_term = (np.sum(W1 ** 2) + np.sum(np.concatenate(W2 ** 2))) * l / 2\n",
        "  rho_b = np.array([np.average(row) for row in hidden_signal.T])\n",
        "  # rho is already given\n",
        "  third_term = kl_divergence(rho, rho_b) * beta\n",
        "  J = first_term + second_term + third_term\n",
        "\n",
        "  # calculate gradients for the layers separately\n",
        "  first_gradient_common = -(1/N) * np.sum(X - output)\n",
        "  first_gradient_2 = np.matmul(first_gradient_common, hidden_signal)\n",
        "  first_gradient_1 = np.matmul(np.matmul(first_gradient_common, W2), X)\n",
        "  second_gradient_2 = l * W2\n",
        "  second_gradient_1 = l * W1\n",
        "  rho_over_b = np.sum(rho / rho_b)\n",
        "  third_gradient_1 = np.array([-beta * rho_over_b[i] * X[i] for i in range(Lhid)])\n",
        "  J_grad_1 = first_gradient_1 + second_gradient_1\n",
        "  J_grad_2 = first_gradient_1 + second_gradient_1\n"
      ],
      "metadata": {
        "id": "OsxHFmkdVEBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rvs(p,size=1):\n",
        "  uniform = np.random.uniform(low=0, high=1, size=size)\n",
        "  print(uniform)\n",
        "  return np.array([uniform < p], dtype=int)\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(p.shape[0]))"
      ],
      "metadata": {
        "id": "Q1VitiXan0hh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}