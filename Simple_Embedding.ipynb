{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEE443_HW2_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMn84AeWwwai7DLLUw0DHE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnisafazla/Neural_Networks_Implementations/blob/main/Simple_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKaejYvmvP9g"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import linspace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrLgqiXjf19f",
        "outputId": "deaaafa0-55c1-426d-982d-559b2bc27c95"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK_HX_Pv1yr7"
      },
      "source": [
        "vocab = np.zeros((250, 250))\n",
        "np.fill_diagonal(vocab, 1)\n",
        "def one_hot(x):\n",
        "  return vocab[x]\n",
        "def inverse_one_hot(x):\n",
        "  return np.argmax(x)\n",
        "def sigmoid(v):\n",
        "  v = np.clip( v, -500, 500 )\n",
        "  return 1/(1 + np.exp(-v))\n",
        "def sigmoid_derivative(v):\n",
        "  return sigmoid(v) * (1 - sigmoid(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaCd73jaf5gX",
        "outputId": "5606fe4a-8cd3-460c-a7b9-8775a6c100f7"
      },
      "source": [
        "file = h5py.File(\"/content/drive/MyDrive/EEE443/hw2/assign2_data2.h5\", \"r\")\n",
        "trainx = np.array(file['trainx'])\n",
        "traind = np.array(file['traind'])\n",
        "testx = np.array(file['testx'])\n",
        "testd = np.array(file['testd'])\n",
        "testx = np.array(file['testx'])\n",
        "valx = np.array(file['valx'])\n",
        "vald = np.array(file['vald'])\n",
        "\n",
        "print(trainx.shape, testx.shape, testx.shape)\n",
        "trainx -= 1\n",
        "valx -= 1\n",
        "testx -= 1\n",
        "traind -= 1\n",
        "vald -= 1\n",
        "testd -= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(372500, 3) (46500, 3) (46500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftWwqGozvixP"
      },
      "source": [
        "words = file['words']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IcMErqbviVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6636953b-afee-4592-fa5f-2255b5043970"
      },
      "source": [
        "words[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'all'"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbJoaVQFglF3"
      },
      "source": [
        "X_train = np.array([[one_hot(word) for word in row] for row in trainx])\n",
        "X_val = np.array([[one_hot(word) for word in row] for row in valx])\n",
        "X_test = np.array([[one_hot(word) for word in row] for row in testx])\n",
        "Y_train = np.array([one_hot(val) for val in traind])\n",
        "Y_val = np.array([one_hot(val) for val in vald])\n",
        "Y_test = np.array([one_hot(val) for val in testd])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNnTT5GNg7Em",
        "outputId": "41672681-1669-4498-ba42-fe30832c59ae"
      },
      "source": [
        "index = [*range(20)]\n",
        "chosen = np.random.choice(index, 5)\n",
        "np.take(index, chosen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  3, 16, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaWuvejV3O0M",
        "outputId": "53ef3e7a-bb23-4e89-dfe3-ce9c3ad01133"
      },
      "source": [
        "chosen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  3, 16, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkbuC3fLT6RU"
      },
      "source": [
        "class Model(object):\n",
        "  def __init__(self, D, P, learning_rate, alpha=0.85):\n",
        "    self.D = D\n",
        "    self.P = P\n",
        "    self.learning_rate = learning_rate\n",
        "    self.alpha = alpha\n",
        "    self.R = np.random.normal(0, 0.01, 250 * D).reshape((250, D))\n",
        "    self.W_hidden = np.random.normal(0, 0.01, P * (D + 1)).reshape((P, D + 1))\n",
        "    self.W_output = np.random.normal(0, 0.01, 250 * (P + 1)).reshape((250, P + 1))\n",
        "    self.r_R = np.zeros((self.R.shape))\n",
        "    self.r_hidden = np.zeros((self.W_hidden.shape))\n",
        "    self.r_output = np.zeros((self.W_output.shape))\n",
        "\n",
        "  def forward_pass(self, X):\n",
        "    self.X = X\n",
        "    self.embedded = np.matmul(self.X, self.R) # input to the hidden layer. take their avg\n",
        "    self.embedded = np.array([np.append(row, -1) for row in self.embedded])\n",
        "    self.v = np.matmul(self.embedded, self.W_hidden.T)\n",
        "    hidden_output = sigmoid(self.v)  # input to the output layer\n",
        "    hidden_output = np.array([np.append(row, -1) for row in hidden_output])\n",
        "    self.hidden_output = hidden_output\n",
        "    \n",
        "    self.z = np.matmul(self.hidden_output, self.W_output.T)  # input to the softmax function\n",
        "    return softmax(self.z)\n",
        "\n",
        "  def back_propogate(self, derivative, batch_size):\n",
        "    gradient_W_output = np.matmul(derivative.T, self.hidden_output)\n",
        "    self.r_output = self.alpha * self.r_output + (1 - self.alpha) * gradient_W_output ** 2\n",
        "    self.update_W_output = - (self.learning_rate * gradient_W_output / (0.0000000000000001 + np.sqrt(self.r_output))) / batch_size\n",
        "\n",
        "    # now hidden layer\n",
        "    W_output_raw = self.W_output.T[0:self.W_output.shape[-1] - 1].T\n",
        "    delta = np.multiply(np.matmul(derivative, W_output_raw), sigmoid_derivative(self.v)).T\n",
        "    gradient_W_hidden = np.matmul(delta, self.embedded)\n",
        "    self.r_hidden = self.alpha * self.r_hidden + (1 - self.alpha) * gradient_W_hidden ** 2\n",
        "    self.update_W_hidden = - (self.learning_rate * gradient_W_hidden / (0.0000000000000001 + np.sqrt(self.r_hidden))) / batch_size\n",
        "\n",
        "    # now embedding layer\n",
        "    W_hidden_raw = self.W_hidden.T[0:self.W_hidden.shape[-1] - 1].T\n",
        "    gradient_R = np.matmul(self.X.T, np.matmul(delta.T, W_hidden_raw))\n",
        "    self.r_R = self.alpha * self.r_R + (1 - self.alpha) * gradient_R ** 2\n",
        "    self.update_R = - (self.learning_rate * gradient_R / (0.0000000000000001 + np.sqrt(self.r_R))) / batch_size\n",
        "\n",
        "    return self.update_W_output, self.update_W_hidden, self.update_R\n",
        "\n",
        "  def train(self, X_train, Y_train, X_val, Y_val, batch_size, verbose=False):\n",
        "    print('Training => This will take some time')\n",
        "    n = X_train.shape[0]\n",
        "    no_of_batches = n // batch_size\n",
        "    extra = n % batch_size\n",
        "\n",
        "    self.train_error_list = []\n",
        "    self.val_error_list = []\n",
        "    for epoch in range(5):\n",
        "      gradient = 0\n",
        "      train_error = 0\n",
        "      val_error = 0\n",
        "      # shuffle X_train, Y_train\n",
        "      index = [*range(n)]\n",
        "      random.shuffle(index)\n",
        "      X_train_shuffled = np.array([X_train[i] for i in index])\n",
        "      Y_train_shuffled = np.array([Y_train[i] for i in index])\n",
        "\n",
        "      # shuffle validation data\n",
        "      index = [*range(n)]\n",
        "      random.shuffle(index)\n",
        "\n",
        "      X_val_tmp = np.array([X_val[i] for i in index])\n",
        "      Y_val_tmp = np.array([Y_val[i] for i in index])\n",
        "      X1_val = X_val_tmp.T[0]\n",
        "      X2_val = X_val_tmp.T[1]\n",
        "      X3_val = X_val_tmp.T[2] \n",
        "\n",
        "      X1_val = np.array([one_hot(word) for word in X1_val])\n",
        "      X2_val = np.array([one_hot(word) for word in X2_val])\n",
        "      X3_val = np.array([one_hot(word) for word in X3_val])\n",
        "      Y1_val = np.array([one_hot(word) for word in Y_val_tmp])\n",
        "\n",
        "      for i in range(no_of_batches):\n",
        "        update_W_output = 0\n",
        "        update_W_hidden = 0\n",
        "        update_R = 0\n",
        "        if i < no_of_batches - 1:\n",
        "          net_batch_size = batch_size\n",
        "          batch_X = X_train_shuffled[i * batch_size:(i+1) * batch_size]\n",
        "          batch_Y = Y_train_shuffled[i * batch_size:(i+1) * batch_size]\n",
        "          \n",
        "        else:\n",
        "          net_batch_size = batch_size + extra\n",
        "          batch_X = X_train_shuffled[i * batch_size:(i+1) * batch_size + extra]\n",
        "          batch_Y = Y_train_shuffled[i * batch_size:(i+1) * batch_size + extra]\n",
        "\n",
        "        X1 = batch_X.T[0]\n",
        "        X2 = batch_X.T[1]\n",
        "        X3 = batch_X.T[2]\n",
        "\n",
        "        X1 = np.array([one_hot(word) for word in X1])\n",
        "        X2 = np.array([one_hot(word) for word in X2])\n",
        "        X3 = np.array([one_hot(word) for word in X3])\n",
        "        Y = np.array([one_hot(word) for word in batch_Y])\n",
        "\n",
        "        softmax_output = self.forward_pass(X1)\n",
        "        cross_out = cross_entropy(softmax_output, X2)\n",
        "\n",
        "        update_W_output_tmp, update_W_hidden_tmp, update_R_tmp = self.back_propogate(softmax_output - cross_out, net_batch_size)\n",
        "        update_W_output += update_W_output_tmp\n",
        "        update_W_hidden += update_W_hidden_tmp\n",
        "        update_R += update_R_tmp\n",
        "        # don't update yet\n",
        "\n",
        "        softmax_output = self.forward_pass(X2)\n",
        "        out = np.array([np.argmax(row) for row in softmax_output]) # indices of the output words\n",
        "        cross_out = cross_entropy(softmax_output, X3)\n",
        "\n",
        "        update_W_output_tmp, update_W_hidden_tmp, update_R_tmp = self.back_propogate(softmax_output - cross_out, net_batch_size)\n",
        "        update_W_output += update_W_output_tmp\n",
        "        update_W_hidden += update_W_hidden_tmp\n",
        "        update_R += update_R_tmp\n",
        "\n",
        "        softmax_output = self.forward_pass(X3)\n",
        "        out = np.array([np.argmax(row) for row in softmax_output]) # indices of the output words\n",
        "        cross_out = cross_entropy(softmax_output, Y)\n",
        "        train_error += cross_out  # classification error\n",
        "\n",
        "        update_W_output_tmp, update_W_hidden_tmp, update_R_tmp = self.back_propogate(softmax_output - cross_out, net_batch_size)\n",
        "        update_W_output += update_W_output_tmp\n",
        "        update_W_hidden += update_W_hidden_tmp\n",
        "        update_R += update_R_tmp\n",
        "\n",
        "        self.W_output += update_W_output / 3\n",
        "        self.W_hidden += update_W_hidden / 3\n",
        "        self.R += update_R / 3\n",
        "\n",
        "        X1_val_tmp = X1_val\n",
        "        X1_val_tmp = self.forward_pass(X1_val_tmp)\n",
        "        X1_val_tmp = self.forward_pass(X1_val_tmp)\n",
        "        X1_val_tmp = self.forward_pass(X1_val_tmp)    # X1_val_tmp has contribution to the output\n",
        "\n",
        "        X2_val_tmp = X2_val\n",
        "        X2_val_tmp = self.forward_pass(X2_val_tmp)\n",
        "        X2_val_tmp = self.forward_pass(X2_val_tmp)    # X2_val_tmp has contribution to the output  \n",
        "\n",
        "        X3_val_tmp = X3_val\n",
        "        X3_val_tmp = self.forward_pass(X3_val_tmp)    # X3_val_tmp has contribution to the output  \n",
        "\n",
        "        # outputs are the probabilities so we can take their averages\n",
        "        val_output = (X1_val_tmp + X2_val_tmp + X3_val_tmp) / 3\n",
        "        val_error += cross_entropy(val_output, Y1_val)\n",
        "        \n",
        "        if verbose == 2:\n",
        "          print('   => Epoch ', epoch, ' batch ', i, ' out of ', no_of_batches, ' => train_error: ', train_error, \", val_error: \", val_error)\n",
        "      if verbose == 1:\n",
        "        print(\"Epoch \", epoch, \" => train_error: \", train_error, \", val_error: \", val_error)\n",
        "      self.train_error_list.append(train_error)\n",
        "      self.val_error_list.append(val_error)\n",
        "    return self.train_error_list, self.val_error_list\n",
        "\n",
        "  def test(X_test, Y_test):\n",
        "    X1 = X_test.T[0]\n",
        "    X2 = X_test.T[1]\n",
        "    X3 = X_test.T[2]\n",
        "\n",
        "    X1 = np.array([one_hot(word) for word in X1])\n",
        "    X2 = np.array([one_hot(word) for word in X2])\n",
        "    X3 = np.array([one_hot(word) for word in X3])\n",
        "  \n",
        "    X1_tmp = X1\n",
        "    X1_tmp = self.forward_pass(X1_tmp)\n",
        "    X1_tmp = self.forward_pass(X1_tmp)\n",
        "    X1_tmp = self.forward_pass(X1_tmp)    # X1_tmp has contribution to the output\n",
        "\n",
        "    X2_tmp = X2\n",
        "    X2_tmp = self.forward_pass(X2_tmp)\n",
        "    X2_tmp = self.forward_pass(X2_tmp)    # X2_tmp has contribution to the output  \n",
        "\n",
        "    X3_tmp = X3\n",
        "    X3_tmp = self.forward_pass(X3_tmp)    # X3_tmp has contribution to the output  \n",
        "\n",
        "    output = (X1_tmp + X2_tmp + X3_tmp) / 3\n",
        "    error = cross_entropy(output, Y_test)\n",
        "    return error\n",
        "  def predict(self, X):\n",
        "    X1 = X.T[0]\n",
        "    X2 = X.T[1]\n",
        "    X3 = X.T[2]\n",
        "  \n",
        "    X1 = np.array([one_hot(word) for word in X1])\n",
        "    X2 = np.array([one_hot(word) for word in X2])\n",
        "    X3 = np.array([one_hot(word) for word in X3])\n",
        "    X1_tmp = X1\n",
        "    X1_tmp = self.forward_pass(X1_tmp)\n",
        "    X1_tmp = self.forward_pass(X1_tmp)\n",
        "    X1_tmp = self.forward_pass(X1_tmp)    # X1_tmp has contribution to the output\n",
        "\n",
        "    X2_tmp = X2\n",
        "    X2_tmp = self.forward_pass(X2_tmp)\n",
        "    X2_tmp = self.forward_pass(X2_tmp)    # X2_tmp has contribution to the output  \n",
        "\n",
        "    X3_tmp = X3\n",
        "    X3_tmp = self.forward_pass(X3_tmp)    # X3_tmp has contribution to the output \n",
        "    output = (X1_tmp + X2_tmp + X3_tmp) / 3\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX0K9cNUNfPq"
      },
      "source": [
        "# try with different D and P values\n",
        "(D, P, batch_size, learning_rate) = (32, 256, 200, 0.15)\n",
        "# no need to shuffle X and Y because shuffled inside the init function\n",
        "model = Model(D, P, learning_rate)\n",
        "train_error_list, val_error_list = model.train(trainx, traind, valx, vald, batch_size, verbose=1)\n",
        "draw_error_curves((train_error_list, val_error_list))\n",
        "test_error = model.test(testx, testd)\n",
        "print('Cross-entropy error on the test data is : ', test_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFqgWOvBXZ1z"
      },
      "source": [
        "draw_error_curves((model.train_error_list, model.val_error_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUHxFxNaaBFn"
      },
      "source": [
        "R = model.R\n",
        "W_hidden = model.W_hidden\n",
        "W_output = model.W_output\n",
        "# model2 = Model(32, 256, 0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoDmsTlrada_"
      },
      "source": [
        "model2 = Model(32, 256, 0.15)\n",
        "model2.R = R\n",
        "model2.W_hidden = W_hidden\n",
        "model2.W_output = W_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygdf2qVwXnBM",
        "outputId": "116bce9f-4fb4-46ba-84bc-2fc8559a600f"
      },
      "source": [
        "print('Part B) try different trigrams')\n",
        "index = [*range(testd.shape[0])]\n",
        "index_chosen = np.random.choice(index, 5)    # take 5 random trigrams\n",
        "\n",
        "X = np.take(testx, index_chosen, axis=0)\n",
        "Y = np.take(testd, index_chosen)\n",
        "outputs = model2.predict(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part B) try different trigrams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoloiEbkb1RC",
        "outputId": "1d126d0f-0630-4283-f45a-bfc0336fd217"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00386851, 0.00501055, 0.00452654, ..., 0.00358649, 0.00460833,\n",
              "        0.00556633],\n",
              "       [0.00386851, 0.00501055, 0.00452654, ..., 0.00358649, 0.00460833,\n",
              "        0.00556633],\n",
              "       [0.00386851, 0.00501055, 0.00452654, ..., 0.00358649, 0.00460833,\n",
              "        0.00556633],\n",
              "       [0.00386851, 0.00501055, 0.00452654, ..., 0.00358649, 0.00460833,\n",
              "        0.00556633],\n",
              "       [0.00386851, 0.00501055, 0.00452654, ..., 0.00358649, 0.00460833,\n",
              "        0.00556633]])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjCDpOcbbAYc"
      },
      "source": [
        "idx = np.array([np.argpartition(row, -10)[-10:] for row in outputs])\n",
        "indices = np.array([idx[i][np.argsort((-outputs[i])[idx[i]])] for i in range(idx.shape[0])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBwjmXMUcBOX"
      },
      "source": [
        "for i in range(5):\n",
        "  print('trigram: ', words[testx[i][0]], words[testx[i][1]], words[testx[i][2]])\n",
        "  print('10 best outputs: ')\n",
        "  x = indices[i]\n",
        "  for j in range(10):\n",
        "    print(words[x[j]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3KLb7O_cFeR"
      },
      "source": [
        "print('The model gave all the same words each time. This is probably because of training less and that these are all common words.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcJl4yOIYATJ",
        "outputId": "65f766c5-1747-4a0c-8162-074062131d0a"
      },
      "source": [
        "testd[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "143"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ObElth1NpIb"
      },
      "source": [
        "# plot all the error curves\n",
        "def draw_error_curves(error, pdf=None):\n",
        "  x = linspace(0, 1, len(error[0]))\n",
        "  ax = plt.subplot(111)\n",
        "\n",
        "  train = error[0]\n",
        "  val = error[1]\n",
        "\n",
        "  plt.plot(x, train, label = \"train_error\")\n",
        "  plt.plot(x, val, label = \"validation_error\")\n",
        "\n",
        "  plt.xlabel(\"epoch\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.title(\"Error Curve\")\n",
        "  ax.legend()\n",
        "  if pdf != None:\n",
        "    plt.savefig(fig1)\n",
        "  else:\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2APfmlMfFI2"
      },
      "source": [
        "def cross_entropy(Y, D, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
        "    and predictions. \n",
        "    Input: predictions (N, k) ndarray\n",
        "           targets (N, k) ndarray        \n",
        "    Returns: scalar\n",
        "    \"\"\"\n",
        "    # D_encode = np.array([one_hot(val) for val in D])\n",
        "    Y = np.clip(Y, epsilon, 1. - epsilon)\n",
        "    N = Y.shape[0]\n",
        "    # print('Y: ', Y.shape)\n",
        "    # print('D: ', D_encode.shape)\n",
        "    ce = -np.sum(D*np.log(Y+1e-9))/N\n",
        "    return ce\n",
        "\n",
        "# SOFTMAX ACTIVATION FUNCTION\n",
        "def softmax(X):\n",
        "  x_max = np.array([np.max(row) for row in X])\n",
        "  sum = np.array([np.sum(np.exp(row - np.max(row))) for row in X])\n",
        "  # print(sum)\n",
        "  prob = np.array([np.exp(X[i] - np.max(X[i])) / sum[i] for i in range(X.shape[0])])\n",
        "  return prob\n",
        "def softmax_crossentropy_derivative(s, y):\n",
        "  # s: output of the softmax\n",
        "  # y: output of the crossentropy\n",
        "  return s - y"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}